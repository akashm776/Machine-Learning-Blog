[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Clustering/index.html",
    "href": "posts/Clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Introduction:\nClustering is a type of unsupervised machine learning technique where the goal is to group similar data points together. The goal of clustering is to identify underlying patterns in a dataset without known labels.\nClustering algorithms uses a similarity metric to measure likeness of data points. Common metrics include Euclidean distance, cosine similarity, or other distance measures. A clustering model’s is evaluated using metrics like silhouette score or Davies-Bouldin index. However, since clustering is unsupervised, evaluation can be subjective and depends on the context of the problem.\nClustering finds applications in various domains, including customer segmentation, anomaly detection, document clustering, image segmentation, and more. It helps discover hidden patterns and structures in data.\n\n\nMethods of Clustering\n\nHierarchical Clustering: Nested clusters that reflect similarity between other clusters. There are two types of Hierarchical Clustering:\n\nAgglomerative / Bottom-Up Clustering, in which we recursively merge similar clusters\nDivisive / Top-Down Clustering, in which we recursively sub-divide into dissimilar sub clusters\n\nK-Clustering: In k-clustering we need to specify the number of clusters we want the data to be grouped into. The algorithm randomly assigns each observation to a cluster and finds the centroid of each cluster. This algorithms iterates through two steps repeatedly until cluster variation cannot be reduced any further.\n\nReassign points to the cluster whose centroid is closest\nCalculate new centroid of each cluster\n\n\n\n\nExample of K-Clustering\nFor the sake of demonstration I will make our own data in order to show clustering.\nCreating Data\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.datasets import make_blobs\ndata = make_blobs(n_samples = 200, n_features = 2, centers=4, cluster_std=1.8, random_state=101)\n\n\nVisulaize the Data\n\n\nCode\nplt.scatter(data[0][:,0],data[0][:,1],c=data[1], cmap='rainbow')\n\n\n&lt;matplotlib.collections.PathCollection at 0x161bda580&gt;\n\n\n\n\n\nCreating Clusters\n\n\nCode\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=4)\nkmeans.fit(data[0])\nkmeans.cluster_centers_\nkmeans.labels_\n\n\n/Users/akashmittal/Desktop/akashm/VT/Fall 2023/CS 5805/Project/.venv/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\narray([1, 3, 2, 3, 3, 0, 3, 2, 3, 2, 1, 2, 3, 3, 1, 2, 3, 2, 0, 1, 0, 2,\n       2, 0, 1, 0, 0, 2, 3, 3, 1, 0, 3, 2, 2, 1, 0, 0, 0, 2, 0, 1, 1, 1,\n       2, 3, 1, 2, 0, 2, 2, 1, 3, 2, 0, 1, 2, 2, 1, 3, 0, 3, 0, 1, 3, 2,\n       0, 3, 3, 0, 3, 2, 0, 2, 0, 3, 3, 2, 1, 2, 2, 0, 3, 0, 2, 2, 2, 1,\n       2, 0, 0, 0, 0, 2, 2, 0, 3, 1, 0, 3, 2, 0, 2, 2, 3, 2, 0, 3, 0, 0,\n       3, 1, 1, 3, 0, 3, 1, 1, 3, 1, 2, 1, 2, 1, 2, 3, 1, 2, 0, 1, 1, 1,\n       2, 0, 0, 1, 3, 1, 3, 2, 0, 3, 0, 1, 1, 3, 2, 0, 1, 1, 1, 1, 2, 3,\n       2, 1, 3, 3, 3, 2, 3, 2, 2, 1, 0, 1, 2, 3, 1, 2, 3, 2, 1, 3, 2, 1,\n       3, 3, 0, 3, 1, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 3, 3, 1, 0, 2, 3, 3,\n       0, 2], dtype=int32)\n\n\n\n\nCode\nf, (ax1, ax2) = plt.subplots(1,2, sharey=True, figsize=(10,6))\nax1.set_title('K-Clustering')\nax1.scatter(data[0][:,0],data[0][:,1],c=kmeans.labels_,cmap='rainbow')\nax2.set_title(\"Original\")\nax2.scatter(data[0][:,0], data[0][:,1], c=data[1],cmap ='rainbow')\n\n\n&lt;matplotlib.collections.PathCollection at 0x16509cc70&gt;"
  },
  {
    "objectID": "posts/Linear and Nonlinear Regression/index.html",
    "href": "posts/Linear and Nonlinear Regression/index.html",
    "title": "Linear and NonLinear Regression",
    "section": "",
    "text": "Introduction to Linear Regression\nLinear regression is a supervised learning algorithm used for predicting a continuous outcome variable (also called the dependent variable) based on one or more predictor variables (independent variables). There is an assumption of a linear relation between the independent predictor variables and the dependent outcome variables.\nIt can be mathematically represented as follows:\n\\(Y = \\mathcal{B}_0 + \\mathcal{B}_1X_1 +\\mathcal{B}_2X_2 + \\mathcal{B}_3X_3 + ... + \\mathcal{B}_nX_n + \\epsilon\\)\nwhere Y is the predicted outcome, \\(X_1\\), \\(X_2\\), …, \\(X_n\\) are predictor variables, \\(\\mathcal{B}_0, \\mathcal{B}_1, \\mathcal{B}_2, \\ldots , \\mathcal{B}_n\\) are weights associated to each predicted variables and \\(\\epsilon\\) is the error term.\nThe goal is to find all \\(\\mathcal{B}_0, \\mathcal{B}_1, \\mathcal{B}_2, \\ldots , \\mathcal{B}_n\\) that minimize the sum of squared differences between the predicted and actual values (least squares method).\nA few assumptions in linear regression other than the linear relationship are the following:\n\nLinear Regression assumes that the residuals (the differences between predicted and actual values) are normally distributed\nThe residuals should have constant variance (homoscedasticity).\nThere should be little or no multicollinearity (high correlation) among the predictor variables.\n\n\n\nIntroduction to NonLinear Regression\nThe relationship between the relationship between the dependent variable and the independent variables is modeled using a nonlinear function.\nThis can be mathematically represented as following:\n\\(Y = f(X, \\mathcal{B}) +\\epsilon\\)\nSimilar to linear regression, the objective in nonlinear regression is often to minimize the difference between the predicted values from the model and the actual observed values. Also evaluation metrics for nonlinear regression are similar to those used in linear regression, such as Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) and etc.\nSome Types of NonLinear relationships can be the following:\n\nQuadratic\nLogarithmic\nExponential\n\n\n\nExample of Linear Regression\nFor this examples we will look real estate houses and their prices, one of the quintessential applications of Linear Regression. The goal is to predict the price of a house based on some features of a house. This data is a dataset taken from Kaggle as toy example to show this concept.\nLoading the Dataset\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\n%matplotlib inline\n\n\n\n\nCode\nUSAhousing = pd.read_csv('USA_Housing.csv')\nUSAhousing.head()\n\n\n\n\n\n\n\n\n\nAvg. Area Income\nAvg. Area House Age\nAvg. Area Number of Rooms\nAvg. Area Number of Bedrooms\nArea Population\nPrice\nAddress\n\n\n\n\n0\n79545.458574\n5.682861\n7.009188\n4.09\n23086.800503\n1.059034e+06\n208 Michael Ferry Apt. 674\\nLaurabury, NE 3701...\n\n\n1\n79248.642455\n6.002900\n6.730821\n3.09\n40173.072174\n1.505891e+06\n188 Johnson Views Suite 079\\nLake Kathleen, CA...\n\n\n2\n61287.067179\n5.865890\n8.512727\n5.13\n36882.159400\n1.058988e+06\n9127 Elizabeth Stravenue\\nDanieltown, WI 06482...\n\n\n3\n63345.240046\n7.188236\n5.586729\n3.26\n34310.242831\n1.260617e+06\nUSS Barnett\\nFPO AP 44820\n\n\n4\n59982.197226\n5.040555\n7.839388\n4.23\n26354.109472\n6.309435e+05\nUSNS Raymond\\nFPO AE 09386\n\n\n\n\n\n\n\nVisualizing the data (which factors are linearly related to Price\n\n\nCode\nsns.pairplot(USAhousing, y_vars = 'Price')\n\n\n\n\n\nTraining Linear Regression Model\n\n\nCode\nX = USAhousing[['Avg. Area Income', 'Avg. Area House Age', 'Avg. Area Number of Rooms', 'Avg. Area Number of Bedrooms', 'Area Population']]\ny = USAhousing['Price']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 101)\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\n\nModel Evaluation - The weights associated to the features\n\n\nCode\nprint(lm.intercept_)\ncoeff_df = pd.DataFrame(lm.coef_, X.columns,columns=['Coefficent'])\ncoeff_df\n\n\n-2640159.796851912\n\n\n\n\n\n\n\n\n\nCoefficent\n\n\n\n\nAvg. Area Income\n21.528276\n\n\nAvg. Area House Age\n164883.282027\n\n\nAvg. Area Number of Rooms\n122368.678027\n\n\nAvg. Area Number of Bedrooms\n2233.801864\n\n\nArea Population\n15.150420\n\n\n\n\n\n\n\nPredictions from the Model\n\n\nCode\npredictions = lm.predict(X_test)\nplt.scatter(y_test,predictions)\n\n\n&lt;matplotlib.collections.PathCollection at 0x147c2c3a0&gt;\n\n\n\n\n\nMetrics to Evaluate the Model\n\n\nCode\nprint('Mean Squared Error = ', metrics.mean_squared_error(y_test,predictions))\n\n\nMean Squared Error =  10460958907.2095"
  },
  {
    "objectID": "posts/Classification/index.html",
    "href": "posts/Classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "Introduction to Classifcation Algorithms in Machine Learning\nClassification is a type of problem in supervised machine learning where the goal is to predict the class labels of new instances based on past observations. In a classification problem, the algorithm learns from a labeled dataset, where each data point is associated with a specific class label. The trained model can then be used to predict the class labels of unseen or future instances.\nThere are two types of classfications based on the number of classes:\n\nBinary Classification: There are two classes to classify into. Usually the classes are True or False or 0 or 1.\nMulticlass Classification: In multiclass classification, there are more than two classes.\n\nSome popular classfication algorithms :\n\nLogistic Regression: Used for binary classification. It models the probability of an instance belonging to a particular class.\nSupport Vector Machines (SVM): SVM seeks to find a hyperplane that best separates different classes in the feature space. It is effective in both binary and multiclass classification.\nDecision Trees: Decision trees recursively split the dataset based on features to create a tree-like structure, making decisions at each node to classify instances.\nk-Nearest Neighbors (k-NN): k-NN classifies instances based on the majority class of their k nearest neighbors in the feature space.\n\nClassfication algorithms are evaluated based on accuracy, precision, recall, F1 score, and the area under the Receiver Operating Characteristic (ROC) curve. But the choice of which metric to choose depends on the specific goals and characteristics of the problem at hand.\n\n\nExample of Logistic Regression\nIn this example I am using Iris dataset which is a popular dataset used in kaggle competitions. In this example we will apply logistic regression to classify iris flowers into two classes (binary classification).\nLoading the Data\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\nimport matplotlib.pyplot as plt\n\niris = sns.load_dataset('iris')\n\niris.head()\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\nVisualize the data\n\n\nCode\nsns.pairplot(iris, hue='species', markers=[\"o\", \"s\", \"D\"], palette='Spectral')\nplt.show()\n\n\n\n\n\nCreating a Logistic Regression Model\n\n\nCode\niris['target'] = np.where(iris['species'] == 'versicolor', 1, 0)\nX = iris[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\ny = iris['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\n\nEvaluating the Model\n\n\nCode\naccuracy = accuracy_score(y_test, y_pred)\nclassification_rep = classification_report(y_test, y_pred)\n\nprint(f\"Accuracy: {accuracy}\")\nprint(\"Classification Report:\\n\", classification_rep)\n\n\nAccuracy: 0.8\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.83      0.90      0.86        21\n           1       0.71      0.56      0.63         9\n\n    accuracy                           0.80        30\n   macro avg       0.77      0.73      0.74        30\nweighted avg       0.79      0.80      0.79        30"
  },
  {
    "objectID": "01_the_machine_learning_landscape.html",
    "href": "01_the_machine_learning_landscape.html",
    "title": "Setup",
    "section": "",
    "text": "Chapter 1 – The Machine Learning landscape\nThis notebook contains the code examples in chapter 1. You’ll also find the exercise solutions at the end of the notebook. The rest of this notebook is used to generate lifesat.csv from the original data sources, and some of this chapter’s figures.\nYou’re welcome to go through the code in this notebook if you want, but the real action starts in the next chapter.\nThis project requires Python 3.7 or above:\nCode\nimport sys\n\nassert sys.version_info &gt;= (3, 7)\nScikit-Learn ≥1.0.1 is required:\nCode\nfrom packaging import version\nimport sklearn\n\nassert version.parse(sklearn.__version__) &gt;= version.parse(\"1.0.1\")\nLet’s define the default font sizes, to plot pretty figures:\nCode\nimport matplotlib.pyplot as plt\n\nplt.rc('font', size=12)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('legend', fontsize=12)\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)\nMake this notebook’s output stable across runs:\nCode\nimport numpy as np\n\nnp.random.seed(42)"
  },
  {
    "objectID": "01_the_machine_learning_landscape.html#load-and-prepare-life-satisfaction-data",
    "href": "01_the_machine_learning_landscape.html#load-and-prepare-life-satisfaction-data",
    "title": "Setup",
    "section": "Load and prepare Life satisfaction data",
    "text": "Load and prepare Life satisfaction data\nTo create lifesat.csv, I downloaded the Better Life Index (BLI) data from OECD’s website (to get the Life Satisfaction for each country), and World Bank GDP per capita data from OurWorldInData.org. The BLI data is in datasets/lifesat/oecd_bli.csv (data from 2020), and the GDP per capita data is in datasets/lifesat/gdp_per_capita.csv (data up to 2020).\nIf you want to grab the latest versions, please feel free to do so. However, there may be some changes (e.g., in the column names, or different countries missing data), so be prepared to have to tweak the code.\n\n\nCode\nimport urllib.request\n\ndatapath = Path() / \"datasets\" / \"lifesat\"\ndatapath.mkdir(parents=True, exist_ok=True)\n\ndata_root = \"https://github.com/ageron/data/raw/main/\"\nfor filename in (\"oecd_bli.csv\", \"gdp_per_capita.csv\"):\n    if not (datapath / filename).is_file():\n        print(\"Downloading\", filename)\n        url = data_root + \"lifesat/\" + filename\n        urllib.request.urlretrieve(url, datapath / filename)\n\n\n\n\nCode\noecd_bli = pd.read_csv(datapath / \"oecd_bli.csv\")\ngdp_per_capita = pd.read_csv(datapath / \"gdp_per_capita.csv\")\n\n\nPreprocess the GDP per capita data to keep only the year 2020:\n\n\nCode\ngdp_year = 2020\ngdppc_col = \"GDP per capita (USD)\"\nlifesat_col = \"Life satisfaction\"\n\ngdp_per_capita = gdp_per_capita[gdp_per_capita[\"Year\"] == gdp_year]\ngdp_per_capita = gdp_per_capita.drop([\"Code\", \"Year\"], axis=1)\ngdp_per_capita.columns = [\"Country\", gdppc_col]\ngdp_per_capita.set_index(\"Country\", inplace=True)\n\ngdp_per_capita.head()\n\n\n\n\n\n\n\n\n\nGDP per capita (USD)\n\n\nCountry\n\n\n\n\n\nAfghanistan\n1978.961579\n\n\nAfrica Eastern and Southern\n3387.594670\n\n\nAfrica Western and Central\n4003.158913\n\n\nAlbania\n13295.410885\n\n\nAlgeria\n10681.679297\n\n\n\n\n\n\n\nPreprocess the OECD BLI data to keep only the Life satisfaction column:\n\n\nCode\noecd_bli = oecd_bli[oecd_bli[\"INEQUALITY\"]==\"TOT\"]\noecd_bli = oecd_bli.pivot(index=\"Country\", columns=\"Indicator\", values=\"Value\")\n\noecd_bli.head()\n\n\n\n\n\n\n\n\nIndicator\nAir pollution\nDwellings without basic facilities\nEducational attainment\nEmployees working very long hours\nEmployment rate\nFeeling safe walking alone at night\nHomicide rate\nHousehold net adjusted disposable income\nHousehold net wealth\nHousing expenditure\n...\nPersonal earnings\nQuality of support network\nRooms per person\nSelf-reported health\nStakeholder engagement for developing regulations\nStudent skills\nTime devoted to leisure and personal care\nVoter turnout\nWater quality\nYears in education\n\n\nCountry\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAustralia\n5.0\nNaN\n81.0\n13.04\n73.0\n63.5\n1.1\n32759.0\n427064.0\n20.0\n...\n49126.0\n95.0\nNaN\n85.0\n2.7\n502.0\n14.35\n91.0\n93.0\n21.0\n\n\nAustria\n16.0\n0.9\n85.0\n6.66\n72.0\n80.6\n0.5\n33541.0\n308325.0\n21.0\n...\n50349.0\n92.0\n1.6\n70.0\n1.3\n492.0\n14.55\n80.0\n92.0\n17.0\n\n\nBelgium\n15.0\n1.9\n77.0\n4.75\n63.0\n70.1\n1.0\n30364.0\n386006.0\n21.0\n...\n49675.0\n91.0\n2.2\n74.0\n2.0\n503.0\n15.70\n89.0\n84.0\n19.3\n\n\nBrazil\n10.0\n6.7\n49.0\n7.13\n61.0\n35.6\n26.7\nNaN\nNaN\nNaN\n...\nNaN\n90.0\nNaN\nNaN\n2.2\n395.0\nNaN\n79.0\n73.0\n16.2\n\n\nCanada\n7.0\n0.2\n91.0\n3.69\n73.0\n82.2\n1.3\n30854.0\n423849.0\n22.0\n...\n47622.0\n93.0\n2.6\n88.0\n2.9\n523.0\n14.56\n68.0\n91.0\n17.3\n\n\n\n\n5 rows × 24 columns\n\n\n\nNow let’s merge the life satisfaction data and the GDP per capita data, keeping only the GDP per capita and Life satisfaction columns:\n\n\nCode\nfull_country_stats = pd.merge(left=oecd_bli, right=gdp_per_capita,\n                              left_index=True, right_index=True)\nfull_country_stats.sort_values(by=gdppc_col, inplace=True)\nfull_country_stats = full_country_stats[[gdppc_col, lifesat_col]]\n\nfull_country_stats.head()\n\n\n\n\n\n\n\n\n\nGDP per capita (USD)\nLife satisfaction\n\n\nCountry\n\n\n\n\n\n\nSouth Africa\n11466.189672\n4.7\n\n\nColombia\n13441.492952\n6.3\n\n\nBrazil\n14063.982505\n6.4\n\n\nMexico\n17887.750736\n6.5\n\n\nChile\n23324.524751\n6.5\n\n\n\n\n\n\n\nTo illustrate the risk of overfitting, I use only part of the data in most figures (all countries with a GDP per capita between min_gdp and max_gdp). Later in the chapter I reveal the missing countries, and show that they don’t follow the same linear trend at all.\n\n\nCode\nmin_gdp = 23_500\nmax_gdp = 62_500\n\ncountry_stats = full_country_stats[(full_country_stats[gdppc_col] &gt;= min_gdp) &\n                                   (full_country_stats[gdppc_col] &lt;= max_gdp)]\ncountry_stats.head()\n\n\n\n\n\n\n\n\n\nGDP per capita (USD)\nLife satisfaction\n\n\nCountry\n\n\n\n\n\n\nRussia\n26456.387938\n5.8\n\n\nGreece\n27287.083401\n5.4\n\n\nTurkey\n28384.987785\n5.5\n\n\nLatvia\n29932.493910\n5.9\n\n\nHungary\n31007.768407\n5.6\n\n\n\n\n\n\n\n\n\nCode\ncountry_stats.to_csv(datapath / \"lifesat.csv\")\nfull_country_stats.to_csv(datapath / \"lifesat_full.csv\")\n\n\n\n\nCode\ncountry_stats.plot(kind='scatter', figsize=(5, 3), grid=True,\n                   x=gdppc_col, y=lifesat_col)\n\nmin_life_sat = 4\nmax_life_sat = 9\n\nposition_text = {\n    \"Turkey\": (29_500, 4.2),\n    \"Hungary\": (28_000, 6.9),\n    \"France\": (40_000, 5),\n    \"New Zealand\": (28_000, 8.2),\n    \"Australia\": (50_000, 5.5),\n    \"United States\": (59_000, 5.3),\n    \"Denmark\": (46_000, 8.5)\n}\n\nfor country, pos_text in position_text.items():\n    pos_data_x = country_stats[gdppc_col].loc[country]\n    pos_data_y = country_stats[lifesat_col].loc[country]\n    country = \"U.S.\" if country == \"United States\" else country\n    plt.annotate(country, xy=(pos_data_x, pos_data_y),\n                 xytext=pos_text, fontsize=12,\n                 arrowprops=dict(facecolor='black', width=0.5,\n                                 shrink=0.08, headwidth=5))\n    plt.plot(pos_data_x, pos_data_y, \"ro\")\n\nplt.axis([min_gdp, max_gdp, min_life_sat, max_life_sat])\n\nsave_fig('money_happy_scatterplot')\nplt.show()\n\n\n\n\n\n\n\nCode\nhighlighted_countries = country_stats.loc[list(position_text.keys())]\nhighlighted_countries[[gdppc_col, lifesat_col]].sort_values(by=gdppc_col)\n\n\n\n\n\n\n\n\n\nGDP per capita (USD)\nLife satisfaction\n\n\nCountry\n\n\n\n\n\n\nTurkey\n28384.987785\n5.5\n\n\nHungary\n31007.768407\n5.6\n\n\nFrance\n42025.617373\n6.5\n\n\nNew Zealand\n42404.393738\n7.3\n\n\nAustralia\n48697.837028\n7.3\n\n\nDenmark\n55938.212809\n7.6\n\n\nUnited States\n60235.728492\n6.9\n\n\n\n\n\n\n\n\n\nCode\ncountry_stats.plot(kind='scatter', figsize=(5, 3), grid=True,\n                   x=gdppc_col, y=lifesat_col)\n\nX = np.linspace(min_gdp, max_gdp, 1000)\n\nw1, w2 = 4.2, 0\nplt.plot(X, w1 + w2 * 1e-5 * X, \"r\")\nplt.text(40_000, 4.9, fr\"$\\theta_0 = {w1}$\", color=\"r\")\nplt.text(40_000, 4.4, fr\"$\\theta_1 = {w2}$\", color=\"r\")\n\nw1, w2 = 10, -9\nplt.plot(X, w1 + w2 * 1e-5 * X, \"g\")\nplt.text(26_000, 8.5, fr\"$\\theta_0 = {w1}$\", color=\"g\")\nplt.text(26_000, 8.0, fr\"$\\theta_1 = {w2} \\times 10^{{-5}}$\", color=\"g\")\n\nw1, w2 = 3, 8\nplt.plot(X, w1 + w2 * 1e-5 * X, \"b\")\nplt.text(48_000, 8.5, fr\"$\\theta_0 = {w1}$\", color=\"b\")\nplt.text(48_000, 8.0, fr\"$\\theta_1 = {w2} \\times 10^{{-5}}$\", color=\"b\")\n\nplt.axis([min_gdp, max_gdp, min_life_sat, max_life_sat])\n\nsave_fig('tweaking_model_params_plot')\nplt.show()\n\n\n\n\n\n\n\nCode\nfrom sklearn import linear_model\n\nX_sample = country_stats[[gdppc_col]].values\ny_sample = country_stats[[lifesat_col]].values\n\nlin1 = linear_model.LinearRegression()\nlin1.fit(X_sample, y_sample)\n\nt0, t1 = lin1.intercept_[0], lin1.coef_[0][0]\nprint(f\"θ0={t0:.2f}, θ1={t1:.2e}\")\n\n\nθ0=3.75, θ1=6.78e-05\n\n\n\n\nCode\ncountry_stats.plot(kind='scatter', figsize=(5, 3), grid=True,\n                   x=gdppc_col, y=lifesat_col)\n\nX = np.linspace(min_gdp, max_gdp, 1000)\nplt.plot(X, t0 + t1 * X, \"b\")\n\nplt.text(max_gdp - 20_000, min_life_sat + 1.9,\n         fr\"$\\theta_0 = {t0:.2f}$\", color=\"b\")\nplt.text(max_gdp - 20_000, min_life_sat + 1.3,\n         fr\"$\\theta_1 = {t1 * 1e5:.2f} \\times 10^{{-5}}$\", color=\"b\")\n\nplt.axis([min_gdp, max_gdp, min_life_sat, max_life_sat])\n\nsave_fig('best_fit_model_plot')\nplt.show()\n\n\n\n\n\n\n\nCode\ncyprus_gdp_per_capita = gdp_per_capita[gdppc_col].loc[\"Cyprus\"]\ncyprus_gdp_per_capita\n\n\n37655.1803457421\n\n\n\n\nCode\ncyprus_predicted_life_satisfaction = lin1.predict([[cyprus_gdp_per_capita]])[0, 0]\ncyprus_predicted_life_satisfaction\n\n\n6.301656332738055\n\n\n\n\nCode\ncountry_stats.plot(kind='scatter', figsize=(5, 3), grid=True,\n                   x=gdppc_col, y=lifesat_col)\n\nX = np.linspace(min_gdp, max_gdp, 1000)\nplt.plot(X, t0 + t1 * X, \"b\")\n\nplt.text(min_gdp + 22_000, max_life_sat - 1.1,\n         fr\"$\\theta_0 = {t0:.2f}$\", color=\"b\")\nplt.text(min_gdp + 22_000, max_life_sat - 0.6,\n         fr\"$\\theta_1 = {t1 * 1e5:.2f} \\times 10^{{-5}}$\", color=\"b\")\n\nplt.plot([cyprus_gdp_per_capita, cyprus_gdp_per_capita],\n         [min_life_sat, cyprus_predicted_life_satisfaction], \"r--\")\nplt.text(cyprus_gdp_per_capita + 1000, 5.0,\n         fr\"Prediction = {cyprus_predicted_life_satisfaction:.2f}\", color=\"r\")\nplt.plot(cyprus_gdp_per_capita, cyprus_predicted_life_satisfaction, \"ro\")\n\nplt.axis([min_gdp, max_gdp, min_life_sat, max_life_sat])\n\nplt.show()\n\n\n\n\n\n\n\nCode\nmissing_data = full_country_stats[(full_country_stats[gdppc_col] &lt; min_gdp) |\n                                  (full_country_stats[gdppc_col] &gt; max_gdp)]\nmissing_data\n\n\n\n\n\n\n\n\n\nGDP per capita (USD)\nLife satisfaction\n\n\nCountry\n\n\n\n\n\n\nSouth Africa\n11466.189672\n4.7\n\n\nColombia\n13441.492952\n6.3\n\n\nBrazil\n14063.982505\n6.4\n\n\nMexico\n17887.750736\n6.5\n\n\nChile\n23324.524751\n6.5\n\n\nNorway\n63585.903514\n7.6\n\n\nSwitzerland\n68393.306004\n7.5\n\n\nIreland\n89688.956958\n7.0\n\n\nLuxembourg\n110261.157353\n6.9\n\n\n\n\n\n\n\n\n\nCode\nposition_text_missing_countries = {\n    \"South Africa\": (20_000, 4.2),\n    \"Colombia\": (6_000, 8.2),\n    \"Brazil\": (18_000, 7.8),\n    \"Mexico\": (24_000, 7.4),\n    \"Chile\": (30_000, 7.0),\n    \"Norway\": (51_000, 6.2),\n    \"Switzerland\": (62_000, 5.7),\n    \"Ireland\": (81_000, 5.2),\n    \"Luxembourg\": (92_000, 4.7),\n}\n\n\n\n\nCode\nfull_country_stats.plot(kind='scatter', figsize=(8, 3),\n                        x=gdppc_col, y=lifesat_col, grid=True)\n\nfor country, pos_text in position_text_missing_countries.items():\n    pos_data_x, pos_data_y = missing_data.loc[country]\n    plt.annotate(country, xy=(pos_data_x, pos_data_y),\n                 xytext=pos_text, fontsize=12,\n                 arrowprops=dict(facecolor='black', width=0.5,\n                                 shrink=0.08, headwidth=5))\n    plt.plot(pos_data_x, pos_data_y, \"rs\")\n\nX = np.linspace(0, 115_000, 1000)\nplt.plot(X, t0 + t1 * X, \"b:\")\n\nlin_reg_full = linear_model.LinearRegression()\nXfull = np.c_[full_country_stats[gdppc_col]]\nyfull = np.c_[full_country_stats[lifesat_col]]\nlin_reg_full.fit(Xfull, yfull)\n\nt0full, t1full = lin_reg_full.intercept_[0], lin_reg_full.coef_[0][0]\nX = np.linspace(0, 115_000, 1000)\nplt.plot(X, t0full + t1full * X, \"k\")\n\nplt.axis([0, 115_000, min_life_sat, max_life_sat])\n\nsave_fig('representative_training_data_scatterplot')\nplt.show()\n\n\n\n\n\n\n\nCode\nfrom sklearn import preprocessing\nfrom sklearn import pipeline\n\nfull_country_stats.plot(kind='scatter', figsize=(8, 3),\n                        x=gdppc_col, y=lifesat_col, grid=True)\n\npoly = preprocessing.PolynomialFeatures(degree=10, include_bias=False)\nscaler = preprocessing.StandardScaler()\nlin_reg2 = linear_model.LinearRegression()\n\npipeline_reg = pipeline.Pipeline([\n    ('poly', poly),\n    ('scal', scaler),\n    ('lin', lin_reg2)])\npipeline_reg.fit(Xfull, yfull)\ncurve = pipeline_reg.predict(X[:, np.newaxis])\nplt.plot(X, curve)\n\nplt.axis([0, 115_000, min_life_sat, max_life_sat])\n\nsave_fig('overfitting_model_plot')\nplt.show()\n\n\n\n\n\n\n\nCode\nw_countries = [c for c in full_country_stats.index if \"W\" in c.upper()]\nfull_country_stats.loc[w_countries][lifesat_col]\n\n\nCountry\nNew Zealand    7.3\nSweden         7.3\nNorway         7.6\nSwitzerland    7.5\nName: Life satisfaction, dtype: float64\n\n\n\n\nCode\nall_w_countries = [c for c in gdp_per_capita.index if \"W\" in c.upper()]\ngdp_per_capita.loc[all_w_countries].sort_values(by=gdppc_col)\n\n\n\n\n\n\n\n\n\nGDP per capita (USD)\n\n\nCountry\n\n\n\n\n\nMalawi\n1486.778248\n\n\nRwanda\n2098.710362\n\n\nZimbabwe\n2744.690758\n\n\nAfrica Western and Central\n4003.158913\n\n\nPapua New Guinea\n4101.218882\n\n\nLower middle income\n6722.809932\n\n\nEswatini\n8392.717564\n\n\nLow & middle income\n10293.855325\n\n\nArab World\n13753.707307\n\n\nBotswana\n16040.008473\n\n\nWorld\n16194.040310\n\n\nNew Zealand\n42404.393738\n\n\nSweden\n50683.323510\n\n\nNorway\n63585.903514\n\n\nSwitzerland\n68393.306004\n\n\n\n\n\n\n\n\n\nCode\ncountry_stats.plot(kind='scatter', x=gdppc_col, y=lifesat_col, figsize=(8, 3))\nmissing_data.plot(kind='scatter', x=gdppc_col, y=lifesat_col,\n                  marker=\"s\", color=\"r\", grid=True, ax=plt.gca())\n\nX = np.linspace(0, 115_000, 1000)\nplt.plot(X, t0 + t1*X, \"b:\", label=\"Linear model on partial data\")\nplt.plot(X, t0full + t1full * X, \"k-\", label=\"Linear model on all data\")\n\nridge = linear_model.Ridge(alpha=10**9.5)\nX_sample = country_stats[[gdppc_col]]\ny_sample = country_stats[[lifesat_col]]\nridge.fit(X_sample, y_sample)\nt0ridge, t1ridge = ridge.intercept_[0], ridge.coef_[0][0]\nplt.plot(X, t0ridge + t1ridge * X, \"b--\",\n         label=\"Regularized linear model on partial data\")\nplt.legend(loc=\"lower right\")\n\nplt.axis([0, 115_000, min_life_sat, max_life_sat])\n\nsave_fig('ridge_model_plot')\nplt.show()"
  },
  {
    "objectID": "posts/Anomaly/outlier detection/index.html",
    "href": "posts/Anomaly/outlier detection/index.html",
    "title": "Anomaly/outlier detection",
    "section": "",
    "text": "Introduction\nAnomaly detection, also known as outlier detection, is a branch of machine learning that focuses on identifying anomalies. Anomalies could be defined as instances or patterns in a dataset that deviate significantly from the majority of the data.\nAnomaly detection can be performed in both supervised and unsupervised settings. This can be done learning to distinguish between normal and anomalous instances in the case of supervised learning. In the case for unsupervised learning the algorithm works with unlabeled data and aims to identify patterns that deviate from the norm without prior knowledge of anomalies.\nCommon Approaches taken for anomaly detection are the following:\n\nStatistical Methods: These methods often involve defining a statistical model for the normal behavior of the data and identifying instances that significantly deviate from this model.\nMachine Learning Models: Machine learning models, such as Isolation Forests, One-Class SVM (Support Vector Machines), and Autoencoders, can be used for anomaly detection.\nDensity-Based Methods: These methods identify anomalies as instances that have lower density compared to their neighbors. DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is an example of a density-based approach.\n\nCommon metrics for evaluating anomaly detection models include precision, recall, F1 score, and the area under the Receiver Operating Characteristic (ROC) curve.\nOne common aspect in anomaly detection that is different from other optimzation related machine learning methods is the setting of an appropriate threshold. The threshold determines the point beyond which instances are considered anomalies. Often visual reprsentations can help identify this. The choice of the threshold often involves trade-offs between false positives and false negatives.\n\n\nExample of Anomaly Detection using Isolation Forest\nIn this example, the tips dataset which is a preloaded seaborn dataset is used. which can easily be obtained by following the just the code below.\nLoad Dataset\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.ensemble import IsolationForest\nimport matplotlib.pyplot as plt\n\ntips = sns.load_dataset('tips')\n\ntips.head()\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n\n\n\n\n\nVisualize the Dataset\n\n\nCode\nsns.scatterplot(x='total_bill', y='tip', data=tips)\nplt.title('Scatter Plot of Total Bill vs. Tip')\nplt.xlabel('Total Bill')\nplt.ylabel('Tip')\nplt.show()\n\n\n\n\n\nTrain the Isolation Forest model and Predict the Anomalies\n\n\nCode\nX = tips[['total_bill', 'tip']]\nmodel = IsolationForest(contamination=0.05, random_state=42)  \nmodel.fit(X)\ntips['anomaly'] = model.predict(X)\n\n\n*Visualize the Anomalies\n\n\nCode\nplt.figure(figsize=(10, 6))\n\nplt.scatter(tips['total_bill'][tips['anomaly'] == 1], tips['tip'][tips['anomaly'] == 1], c='blue', label='Normal')\n\nplt.scatter(tips['total_bill'][tips['anomaly'] == -1], tips['tip'][tips['anomaly'] == -1], c='red', label='Anomaly')\n\nplt.title('Anomaly Detection with Isolation Forest')\nplt.xlabel('Total Bill')\nplt.ylabel('Tip')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/Probability theory and random variables/index.html",
    "href": "posts/Probability theory and random variables/index.html",
    "title": "Probability Concepts used in Machine Learning",
    "section": "",
    "text": "Machine Learning and Artificial Intelligence are the new buzzwords of the 21st century. Eventhough in casual conversations Machine Learning and Artificial Intellegence are used intechangably, Machine Learning is actually defined to be a sub branch of Artificial Intelligence.\nAccording to IBM, one of the leading industrial researchers of AI and ML, Machine Learning can be broadly defined as the capability of a machine to imitate human behavior.\nBut there are myriad of human behaviors, idiosyncrasies that the machine could try to imitate, so which one is Machine Learning trying to emulate. Broadly speaking machine learning is trying to capture a human’s ability to adapt and learn from their previous experiences and apply those lessons in new unforseen experiences.\nIn the domain of Machine Learning this translates to the following\nExperience = Data\nGoal: Learn from past data trends in order to predict future data values.\nBut as we are all familiar with the act of applying from previous experiences to new experiences is not a straightforward process. There is a degree of uncertanity since every experience is different. Undersatnding this uncertanity is crucial in the domain of Machine Learning, and this is where Probabilty Theory comes in…\nProbability Theory can broadly be defined as the mathematical study of uncertainty."
  },
  {
    "objectID": "posts/Probability theory and random variables/index.html#motivation-to-study-probability-concepts-for-machine-learning",
    "href": "posts/Probability theory and random variables/index.html#motivation-to-study-probability-concepts-for-machine-learning",
    "title": "Probability Concepts used in Machine Learning",
    "section": "",
    "text": "Machine Learning and Artificial Intelligence are the new buzzwords of the 21st century. Eventhough in casual conversations Machine Learning and Artificial Intellegence are used intechangably, Machine Learning is actually defined to be a sub branch of Artificial Intelligence.\nAccording to IBM, one of the leading industrial researchers of AI and ML, Machine Learning can be broadly defined as the capability of a machine to imitate human behavior.\nBut there are myriad of human behaviors, idiosyncrasies that the machine could try to imitate, so which one is Machine Learning trying to emulate. Broadly speaking machine learning is trying to capture a human’s ability to adapt and learn from their previous experiences and apply those lessons in new unforseen experiences.\nIn the domain of Machine Learning this translates to the following\nExperience = Data\nGoal: Learn from past data trends in order to predict future data values.\nBut as we are all familiar with the act of applying from previous experiences to new experiences is not a straightforward process. There is a degree of uncertanity since every experience is different. Undersatnding this uncertanity is crucial in the domain of Machine Learning, and this is where Probabilty Theory comes in…\nProbability Theory can broadly be defined as the mathematical study of uncertainty."
  },
  {
    "objectID": "posts/Probability theory and random variables/index.html#defining-probability-concepts-used-in-machine-learning",
    "href": "posts/Probability theory and random variables/index.html#defining-probability-concepts-used-in-machine-learning",
    "title": "Probability Concepts used in Machine Learning",
    "section": "Defining Probability Concepts Used in Machine Learning",
    "text": "Defining Probability Concepts Used in Machine Learning\n\nProbability Space\nWhen one thinks about probability we often think about the probability of an event of uncertain nature taking place. Therefore, in order to discuss probability theory formally, let’s first clarify what the possible events are to which we would like to attach probability.\nFormally Probability Space refers to these three spaces\n\n\\(\\Omega\\): The space of possible outcomes (Also know as Outcome Space)\nF: The space of measurable events (Also known as the Event Space)\nP: The probability measure (Probability Distribution) which maps an event in the Event Space to a real number between 0 and 1.\n\nAnother concept that is closely associated to Probability Theory is the concept of Random Variable.\n\n\nRandom Variables\nRandom Variable is a bit of a misnomer. They are actually not a variables, but are functions that map outcomes (in the outcome space) to real values (notated as a capital letter). Random variables allows one to abstract away from the formal notion of event space hence allowing one to define random variables that capture the appropriate events.\nA popular notation seen in elementary proabbility course is the following: \\(P(X = a)\\)\nThis means the probability of a random variable \\(X\\) taking on the value of \\(a\\).\n\n\nDistributions\nDistribution of a variable formally refers to the probability of a random variable taking on certain values.\nFor example if a coin is fair then the distribution of \\(X\\) would be the following:\n\\(P(X = Head) = P(X = Tail) = \\frac{1}{2}\\)\nThere are lot of distributions in Probability Theory , but one of the most useful one for Machine Learning is Conditional Distributions.\nConditional distributions are one of the primary concept in probability theory for working with uncertainty. They specify the distribution of a random variable when the value of another random variable is known (or more generally, when some event is known to be true). Conditional Probability is used as follows:\nConditional Probability of \\(X = a\\) given \\(Y = b\\) is defined as \\(P(X =a|Y =b)= \\frac{P(X = a,Y = b)}{P(Y = b)}\\)\nNote: this is not defined when the probability of \\(Y = b\\) is 0\n\n\nIndpendence\nIn probability theory, independence means that the distribution of a random variable does not change on learning the value of another random variable. In machine learning, we often make such assumptions about our data. For example, the training samples are assumed to be drawn independently from some underlying space; the label of sample i is assumed to be independent of the features of sample j.\nA random variable \\(X\\) is said to be independent of \\(Y\\) when \\(P(X) = P(X|Y)\\)\nIn Machine Learning independence is mostly used in the conetxt of conditional independence, meaning that if we know the value of a random variable (or more generally, a set of random variables), then some other random variables will be independent of each other.\nAn example of Conditional Independence is the Naive Bayes assumption which says that all features in the input data are independent of each other.\n\n\nExample of Naive Bayes\nA popular example where Naive bayes is used is a classifier. For this example we will use the very popular Iris dataset which we have used in class. We will use Naive Bayes to classify the iris flowers into different species. Here there is a simplistic assumption made that Sepal length, sepal width, petal length, and petal width are conditionally independent given the species label.\nLoading the dataset{.smaller}\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport matplotlib.pyplot as plt\n\ndata = sns.load_dataset('iris')\n\nprint(data.head)\n\n\n&lt;bound method NDFrame.head of      sepal_length  sepal_width  petal_length  petal_width    species\n0             5.1          3.5           1.4          0.2     setosa\n1             4.9          3.0           1.4          0.2     setosa\n2             4.7          3.2           1.3          0.2     setosa\n3             4.6          3.1           1.5          0.2     setosa\n4             5.0          3.6           1.4          0.2     setosa\n..            ...          ...           ...          ...        ...\n145           6.7          3.0           5.2          2.3  virginica\n146           6.3          2.5           5.0          1.9  virginica\n147           6.5          3.0           5.2          2.0  virginica\n148           6.2          3.4           5.4          2.3  virginica\n149           5.9          3.0           5.1          1.8  virginica\n\n[150 rows x 5 columns]&gt;\n\n\nVisualize the dataset{.smaller}\n\n\nCode\nsns.pairplot(data, hue='species', markers=[\"o\", \"s\", \"D\"])\nplt.show()\n\n\n\n\n\nPreparing the Data{.smaller}\n\n\nCode\nX = data.drop('species', axis=1)\ny = data['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\nTraining the Naive Bayes Classifier{.smaller}\n\n\nCode\nmodel = GaussianNB()\nmodel.fit(X_train, y_train)\n\n\nPredictions{.smaller}\n\n\nCode\ny_pred = model.predict(X_test)\n\n\nEvaluating the Model{.smaller}\n\n\nCode\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\n\nprint(f\"Accuracy: {accuracy}\")\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n\nAccuracy: 1.0\nConfusion Matrix:\n[[10  0  0]\n [ 0  9  0]\n [ 0  0 11]]\n\n\nVisualizing the Confusion Matrix{.smaller}\n\n\nCode\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=model.classes_, yticklabels=model.classes_)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.title('Confusion Matrix')\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Project",
    "section": "",
    "text": "Anomaly/outlier detection\n\n\n\n\n\n\n\nIsolation Forest\n\n\nSupervised Learning\n\n\n\n\nTheory and an example related to Anomaly Detection\n\n\n\n\n\n\nNov 21, 2023\n\n\nAkash Mittal\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\nLogistic Regression\n\n\nSupervised Learning\n\n\n\n\nTheory and example about classfication algorithms\n\n\n\n\n\n\nNov 18, 2023\n\n\nAkash Mittal\n\n\n\n\n\n\n  \n\n\n\n\nLinear and NonLinear Regression\n\n\n\n\n\n\n\nLinear Regression\n\n\nSupervised Learning\n\n\n\n\nTheory on Linear and Non linear regression and an example of linear regression model.\n\n\n\n\n\n\nNov 17, 2023\n\n\nAkash Mittal\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\nK-Clustering\n\n\nNon-Supervised Learning\n\n\n\n\nTheory and example on Clustering techniques\n\n\n\n\n\n\nNov 15, 2023\n\n\nAkash Mittal\n\n\n\n\n\n\n  \n\n\n\n\nProbability Concepts used in Machine Learning\n\n\n\n\n\n\n\nNaive Bayes\n\n\nSupervised Learning\n\n\n\n\nIntroduction to theoretical probability concepts and an example of Naive Bayes.\n\n\n\n\n\n\nNov 13, 2023\n\n\nAkash Mittal\n\n\n\n\n\n\nNo matching items"
  }
]