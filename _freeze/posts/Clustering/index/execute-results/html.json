{
  "hash": "8146a117576a55b355866ba552818eb9",
  "result": {
    "markdown": "---\ntitle: \"Clustering\"\ntitle-block-banner: true\ndescription: \"Theory and example on Clustering techniques\"\ncategories: [K-Clustering, Non-Supervised Learning]\nauthor: \"Akash Mittal\"\ndate: \"11/15/2023\"\n---\n\n### Introduction:\nClustering is a type of unsupervised machine learning technique where the goal is to group similar data points together. The goal of clustering is to identify underlying patterns in a dataset without known labels.\n\nClustering algorithms uses a similarity metric to measure likeness of data points. Common metrics include Euclidean distance, cosine similarity, or other distance measures. A clustering model's is evaluated using metrics like silhouette score or Davies-Bouldin index. However, since clustering is unsupervised, evaluation can be subjective and depends on the context of the problem.\n\nClustering finds applications in various domains, including customer segmentation, anomaly detection, document clustering, image segmentation, and more. It helps discover hidden patterns and structures in data.\n\n### Methods of Clustering \n\n1. **Hierarchical Clustering**: Nested clusters that reflect similarity between other clusters. There are two types of Hierarchical Clustering:\n\n    1. Agglomerative / Bottom-Up Clustering, in which we recursively merge similar clusters\n\n    2. Divisive / Top-Down Clustering, in which we recursively sub-divide into dissimilar sub clusters\n\n2. **K-Clustering**: In k-clustering we need to specify the number of clusters we want the data to be grouped into. The algorithm randomly assigns each observation to a cluster and finds the centroid of each cluster. This algorithms iterates through two steps repeatedly until cluster variation cannot be reduced any further.\n    1. Reassign points to the cluster whose centroid is closest\n    2. Calculate new centroid of each cluster\n\n### Example of K-Clustering\n\nFor the sake of demonstration I will make our own data in order to show clustering. \n\n**Creating Data**\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.datasets import make_blobs\ndata = make_blobs(n_samples = 200, n_features = 2, centers=4, cluster_std=1.8, random_state=101)\n```\n:::\n\n\n**Visulaize the Data**\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nplt.scatter(data[0][:,0],data[0][:,1],c=data[1], cmap='rainbow')\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n<matplotlib.collections.PathCollection at 0x161bda580>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-2.png){width=577 height=411}\n:::\n:::\n\n\n**Creating Clusters**\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=4)\nkmeans.fit(data[0])\nkmeans.cluster_centers_\nkmeans.labels_\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/akashmittal/Desktop/akashm/VT/Fall 2023/CS 5805/Project/.venv/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\narray([1, 3, 2, 3, 3, 0, 3, 2, 3, 2, 1, 2, 3, 3, 1, 2, 3, 2, 0, 1, 0, 2,\n       2, 0, 1, 0, 0, 2, 3, 3, 1, 0, 3, 2, 2, 1, 0, 0, 0, 2, 0, 1, 1, 1,\n       2, 3, 1, 2, 0, 2, 2, 1, 3, 2, 0, 1, 2, 2, 1, 3, 0, 3, 0, 1, 3, 2,\n       0, 3, 3, 0, 3, 2, 0, 2, 0, 3, 3, 2, 1, 2, 2, 0, 3, 0, 2, 2, 2, 1,\n       2, 0, 0, 0, 0, 2, 2, 0, 3, 1, 0, 3, 2, 0, 2, 2, 3, 2, 0, 3, 0, 0,\n       3, 1, 1, 3, 0, 3, 1, 1, 3, 1, 2, 1, 2, 1, 2, 3, 1, 2, 0, 1, 1, 1,\n       2, 0, 0, 1, 3, 1, 3, 2, 0, 3, 0, 1, 1, 3, 2, 0, 1, 1, 1, 1, 2, 3,\n       2, 1, 3, 3, 3, 2, 3, 2, 2, 1, 0, 1, 2, 3, 1, 2, 3, 2, 1, 3, 2, 1,\n       3, 3, 0, 3, 1, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 3, 3, 1, 0, 2, 3, 3,\n       0, 2], dtype=int32)\n```\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nf, (ax1, ax2) = plt.subplots(1,2, sharey=True, figsize=(10,6))\nax1.set_title('K-Clustering')\nax1.scatter(data[0][:,0],data[0][:,1],c=kmeans.labels_,cmap='rainbow')\nax2.set_title(\"Original\")\nax2.scatter(data[0][:,0], data[0][:,1], c=data[1],cmap ='rainbow')\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n<matplotlib.collections.PathCollection at 0x16509cc70>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-2.png){width=801 height=505}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}