{
  "hash": "8d9beba602a62577dab576a5378456bd",
  "result": {
    "markdown": "---\ntitle: \"Probability Concepts used in Machine Learning\"\ntitle-block-banner: true\ndescription: \"Introduction to theoretical probability concepts and an example of Naive Bayes.\"\ncategories: [Naive Bayes, Supervised Learning]\nauthor: \"Akash Mittal\"\ndate: \"11/13/2023\"\n---\n\n## Motivation to Study Probability Concepts for Machine Learning {.larger}\n\nMachine Learning and Artificial Intelligence are the new buzzwords of the 21st century. Eventhough in casual conversations Machine Learning and Artificial Intellegence are used intechangably, Machine Learning is actually defined to be a sub branch of Artificial Intelligence.\n\nAccording to IBM, one of the leading industrial researchers of AI and ML, *Machine Learning can be broadly defined as the capability of a machine to imitate human behavior.* \n\nBut there are myriad of human behaviors, idiosyncrasies that the machine could try to imitate, so which one is Machine Learning trying to emulate. Broadly speaking machine learning is trying to capture a human's ability to adapt and learn from their previous experiences and apply those lessons in new unforseen experiences.\n\nIn the domain of Machine Learning this translates to the following\n\n_Experience = Data_\n\n_Goal: Learn from past data trends in order to predict future data values._\n\nBut as we are all familiar with the act of applying from previous experiences to new experiences is not a straightforward process. There is a degree of uncertanity since every experience is different. Undersatnding this uncertanity is crucial in the domain of Machine Learning, and this is where Probabilty Theory comes in...\n\n_**Probability Theory can broadly be defined as the mathematical study of uncertainty.**_\n\n## Defining Probability Concepts Used in Machine Learning {.larger}\n\n### Probability Space \nWhen one thinks about probability we often think about the probability of an event of uncertain nature taking place. Therefore, in order to discuss probability theory formally, let's first clarify what the possible events are to which we would like to attach probability.\n\nFormally _Probability Space_ refers to these three spaces\n\n1. **$\\Omega$**: The space of possible outcomes (Also know as Outcome Space)\n\n2. **F**: The space of measurable events (Also known as the Event Space)\n\n3. **P**: The probability measure (Probability Distribution) which maps an event in the Event Space to a real number between 0 and 1.\n\nAnother concept that is closely associated to Probability Theory is the concept of **_Random Variable_**. \n\n### Random Variables\n\nRandom Variable is a bit of a misnomer. They are actually not a variables, but are functions that map outcomes (in the outcome space) to real values (notated as a capital letter). Random variables allows one to abstract away from the formal notion of event space hence allowing one to define random variables that capture the appropriate events.\n\nA popular notation seen in elementary proabbility course is the following: **_$P(X = a)$_**\n\nThis means the probability of a random variable $X$ taking on the value of $a$.\n\n### Distributions\n\nDistribution of a variable formally refers to the probability of a random variable taking on certain values.\n\nFor example if a coin is fair then the distribution of $X$ would be the following:\n\n**_$P(X = Head) = P(X = Tail) = \\frac{1}{2}$_**\n\nThere are lot of distributions in Probability Theory , but one of the most useful one for Machine Learning is **_Conditional Distributions_**.\n\nConditional distributions are one of the primary concept in probability theory for working with uncertainty. They specify the distribution of a random variable when the value of another random variable is known (or more generally, when some event is known to be true). Conditional Probability is used as follows:\n\nConditional Probability of $X = a$ given $Y = b$ is defined as\n$P(X =a|Y =b)= \\frac{P(X = a,Y = b)}{P(Y = b)}$ \n\nNote: this is not defined when the probability of $Y = b$ is 0\n\n### Indpendence\n\nIn probability theory, independence means that the distribution of a random variable does not change on learning the value of another random variable. In machine learning, we often make such assumptions about our data. For example, the training samples are assumed to be drawn independently from some underlying space; the label of sample i is assumed to be independent of the features of sample j.\n\nA random variable $X$ is said to be independent of $Y$ when $P(X) = P(X|Y)$\n\nIn Machine Learning independence is mostly used in the conetxt of _conditional independence_, meaning that if we know the value of a random variable (or more generally, a set of random variables), then some other random variables will be independent of each other.\n\nAn example of Conditional Independence is the Naive Bayes assumption which says that all features in the input data are independent of each other.\n\n### Example of Naive Bayes\n\nA popular example where Naive bayes is used is a classifier. For this example we will use the very popular Iris dataset which we have used in class. We will use Naive Bayes to classify the iris flowers into different species. Here there is a simplistic assumption made that Sepal length, sepal width, petal length, and petal width are conditionally independent given the species label.\n\n**Loading the dataset**{.smaller}\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport matplotlib.pyplot as plt\n\ndata = sns.load_dataset('iris')\n\nprint(data.head)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<bound method NDFrame.head of      sepal_length  sepal_width  petal_length  petal_width    species\n0             5.1          3.5           1.4          0.2     setosa\n1             4.9          3.0           1.4          0.2     setosa\n2             4.7          3.2           1.3          0.2     setosa\n3             4.6          3.1           1.5          0.2     setosa\n4             5.0          3.6           1.4          0.2     setosa\n..            ...          ...           ...          ...        ...\n145           6.7          3.0           5.2          2.3  virginica\n146           6.3          2.5           5.0          1.9  virginica\n147           6.5          3.0           5.2          2.0  virginica\n148           6.2          3.4           5.4          2.3  virginica\n149           5.9          3.0           5.1          1.8  virginica\n\n[150 rows x 5 columns]>\n```\n:::\n:::\n\n\n**Visualize the dataset**{.smaller}\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nsns.pairplot(data, hue='species', markers=[\"o\", \"s\", \"D\"])\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=1069 height=947}\n:::\n:::\n\n\n**Preparing the Data**{.smaller}\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nX = data.drop('species', axis=1)\ny = data['species']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n```\n:::\n\n\n**Training the Naive Bayes Classifier**{.smaller}\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nmodel = GaussianNB()\nmodel.fit(X_train, y_train)\n```\n:::\n\n\n**Predictions**{.smaller}\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ny_pred = model.predict(X_test)\n```\n:::\n\n\n**Evaluating the Model**{.smaller}\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\n\nprint(f\"Accuracy: {accuracy}\")\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 1.0\nConfusion Matrix:\n[[10  0  0]\n [ 0  9  0]\n [ 0  0 11]]\n```\n:::\n:::\n\n\n**Visualizing the Confusion Matrix**{.smaller}\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=model.classes_, yticklabels=model.classes_)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.title('Confusion Matrix')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){width=547 height=449}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}